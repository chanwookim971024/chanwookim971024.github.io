---
---
@string{aps = {American Physical Society,}}
@article{kim2025dyn,
      author={Kim, Hyeonseong and Kim, Chanwoo and Pan, Matthew and Lee, Kyungjae and Choi, Sungjoon},
      title={Learning-based Dynamic Robot-to-Human Handover},
      journal={IEEE International Conference on Robotics and Automation (ICRA)},
      abbr={ICRA2025},
      publisher={IEEE},
      year={2025},
      abstract={This paper presents a novel learning-based approach to dynamic robot-to-human handover, addressing the challenges of delivering objects to a moving receiver. We hypothesize that dynamic handover, where the robot adjusts to the receiver's movements, results in more efficient and comfortable interaction compared to static handover, where the receiver is assumed to be stationary. To validate this, we developed a nonparametric method for generating continuous handover motion, conditioned on the receiver's movements, and trained the model using a dataset of 1,000 human-to-human handover demonstrations. We integrated preference learning for improved handover effectiveness and applied impedance control to ensure user safety and adaptiveness. The approach was evaluated in both simulation and real-world settings, with user studies demonstrating that dynamic handover significantly reduces handover time and improves user comfort compared to static methods. Videos and demonstrations of our approach are available at https://zerotohero7886.github.io/dyn-r2h-handover/},
      pdf={https://arxiv.org/abs/2502.12602},
      website={https://zerotohero7886.github.io/dyn-r2h-handover/},
      preview={dyn_ho.gif},
      selected={true},
}

@article{hwang2023sequential,
  title={Sequential preference ranking for efficient reinforcement learning from human feedback},
  author={Hwang, Minyoung and Lee, Gunmin and Kee, Hogun and Kim, Chan Woo and Lee, Kyungjae and Oh, Songhwai},
  journal={Advances in Neural Information Processing Systems (NeurIPS)},
  abbr={NeurIPS2023},
  volume={36},
  pages={49088--49099},
  year={2023},
  abstract={Reinforcement learning from human feedback (RLHF) alleviates the problem of designing a task-specific reward function in reinforcement learning by learning it from human preference. However, existing RLHF models are considered inefficient as they produce only a single preference data from each human feedback. To tackle this problem, we propose a novel RLHF framework called SeqRank, that uses sequential preference ranking to enhance the feedback efficiency. Our method samples trajectories in a sequential manner by iteratively selecting a defender from the set of previously chosen trajectories K and a challenger from the set of unchosen trajectories U \ K, where U is the replay buffer. We propose two trajectory comparison methods with different defender sampling strategies: (1) sequential pairwise comparison that selects the most recent trajectory and (2) root pairwise comparison that selects the most preferred trajectory from K. We construct a data structure and rank trajectories by preference to augment additional queries. The proposed method results in at least 39.2% higher average feedback efficiency than the baseline and also achieves a balance between feedback efficiency and data dependency. We examine the convergence of the empirical risk and the generalization bound of the reward model with Rademacher complexity. While both trajectory comparison methods outperform conventional pairwise comparison, root pairwise comparison improves the average reward in locomotion tasks and the average success rate in manipulation tasks by 29.0% and 25.0%, respectively. Project page: https://rllab-snu.github.io/projects/SeqRank},
  website={https://rllab-snu.github.io/projects/SeqRank/},
  pdf={https://proceedings.neurips.cc/paper_files/paper/2023/file/99766cda865be123d55a1d9666c7b9fc-Paper-Conference.pdf},
  preview={seqrank.png},
  selected={true},
}
